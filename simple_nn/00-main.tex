\item \points{20} {\bf A Simple Neural Network}

Let $X = \{x^{(1)}, \cdots, x^{(\nexp)}\}$ be a dataset of $\nexp$ samples with 2 features, i.e $\xsi \in \R^2$. The samples are classified into 2 categories with labels $\ysi \in \{0, 1\}$. A scatter plot of the dataset is shown in Figure $\ref{fig:nn_plot}$:
\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.3]{simple_nn/nn_plot.png}
    \caption{Plot of dataset $X$.}
    \label{fig:nn_plot}
\end{figure}

The examples in class $1$ are marked as as ``$\times$" and examples in class $0$ are marked as ``$\circ$". We want to perform binary classification using a simple neural network with the architecture shown in Figure $\ref{fig:nn_arc}$:
\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.3, clip]{simple_nn/nn_architecture.png}
    \caption{Architecture for our simple neural network.}
     \label{fig:nn_arc}
\end{figure}

Denote the two features $x_1$ and $x_2$, the three neurons in the hidden layer $h_1, h_2$, and $h_3$, and the output neuron as $o$. Let the weight from $x_i$ to $h_j$ be $w_{i, j}^{[1]}$ for $i \in \{1, 2\}, j \in \{1, 2, 3\}$, and the weight from $h_j$ to $o$ be $w_{j}^{[2]}$. Finally, denote the intercept weight for $h_j$ as $w_{0, j}^{[1]}$, and the intercept weight for $o$ as $w_{0}^{[2]}$. For the loss function, we'll use average squared loss instead of the usual negative log-likelihood:
$$l = \frac{1}{\nexp}\sum_{i=1}^{\nexp} \left(o^{(i)} - \ysi\right)^2,$$
where $o^{(i)}$ is the result of the output neuron for example $i$.

\begin{enumerate}
  \input{simple_nn/01-sigmoid}

\ifnum\solutions=1 {
  \input{simple_nn/01-sigmoid_sol}
} \fi

  \input{simple_nn/02-step_function}

\ifnum\solutions=1 {
  \input{simple_nn/02-step_function_sol}
} \fi

  \input{simple_nn/03-linear}
\ifnum\solutions=1 {
  \input{simple_nn/03-linear_sol}
} \fi


\end{enumerate}
